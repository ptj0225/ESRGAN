{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from models import get_generator, get_discriminator, get_feature_extractor\n",
    "import argparse\n",
    "from glob import glob\n",
    "import warnings\n",
    "import util\n",
    "import albumentations as A\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "  # Invalid device or cannot modify virtual devices once initialized.\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--epochs', required=False, default=1000, help='epochs')\n",
    "parser.add_argument('--batchs', required=False, default=16, help='batchs')\n",
    "parser.add_argument('--lr_g', required=False, default=0.0001, help='learning rate of generator')\n",
    "parser.add_argument('--lr_d', required=False, default=0.00005, help='learning rate of discriminator')\n",
    "parser.add_argument('--train_dir', required=False, default=\"./train/\", help='directory of image to train / 학습 할 이미지 위치')\n",
    "parser.add_argument('--load_model', required=False, default=True, help='load saved model / 저장된 모델 불러오기 (1: True, 0: False)')\n",
    "parser.add_argument('--use_cpu', required=False, default=False, help='forced to use CPU only / CPU 만 이용해 학습하기 (1: True, 0: False)')\n",
    "args = parser.parse_args()\n",
    "\n",
    "epochs = args.epochs\n",
    "batchs = args.batchs\n",
    "lr_g = args.lr_g\n",
    "lr_d = args.lr_d\n",
    "train_dir =  args.train_dir\n",
    "load_model =  args.load_model\n",
    "use_cpu =  args.use_cpu\n",
    "\n",
    "if use_cpu: os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "# 모델 불러오기 or 새로 생성하기\n",
    "if load_model:\n",
    "\n",
    "    if os.path.isfile('Generator.h5'):\n",
    "        Generator= tf.keras.models.load_model('Generator.h5')\n",
    "        print('Generator loaded')\n",
    "    else:\n",
    "        print('Cant load Generator')\n",
    "        Generator = get_generator()\n",
    "\n",
    "    if os.path.isfile('Discriminator.h5'):\n",
    "        Discriminator = tf.keras.models.load_model('Discriminator.h5')\n",
    "        print('Discriminator loaded')\n",
    "    else:\n",
    "        print('Cant load Discriminator')\n",
    "        Discriminator = get_discriminator()\n",
    "\n",
    "else:\n",
    "    Generator = get_generator()\n",
    "    Discriminator = get_discriminator()\n",
    "\n",
    "# feature map 생성을 위한 feature extractor 선언\n",
    "feature_extractor = get_feature_extractor()\n",
    "\n",
    "feature_extractor.trainable = False\n",
    "Discriminator.trainable = True\n",
    "Generator.trainable = True\n",
    "\n",
    "def BGR2RGB(image):\n",
    "    channels = tf.unstack(image, axis=-1)\n",
    "    image    = tf.stack([channels[2], channels[1], channels[0]], axis=-1)\n",
    "    return image\n",
    "\n",
    "imgs = []\n",
    "mae = tf.keras.losses.MeanAbsoluteError()\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "bce = tf.losses.BinaryCrossentropy()\n",
    "optim_g = tf.keras.optimizers.Adam(lr_g, beta_1=0.9, beta_2=0.999)\n",
    "optim_d = tf.keras.optimizers.Adam(lr_d, beta_1=0.9, beta_2=0.999)\n",
    "iter_count = 1\n",
    "im_inx = glob(train_dir + \"*.png\") + glob(train_dir + \"*.jpg\")\n",
    "cv2.startWindowThread()\n",
    "cv2.namedWindow('sample')\n",
    "\n",
    "transform_hr = A.Compose([A.RandomRotate90(p=0.5),\n",
    "                          A.HorizontalFlip(p=0.5)],)\n",
    "transform_lr = A.Compose([\n",
    "                        A.OneOf([A.GaussianBlur(always_apply=True),\n",
    "                                   A.RingingOvershoot(always_apply=True)], p=1),\n",
    "                        A.OneOf([A.Resize(64, 64, always_apply=True, interpolation=cv2.INTER_AREA),\n",
    "                                 A.Resize(64, 64, always_apply=True, interpolation=cv2.INTER_CUBIC),\n",
    "                                 A.Resize(64, 64, always_apply=True, interpolation=cv2.INTER_LINEAR)], p=1),\n",
    "                        A.OneOf([A.GaussNoise(always_apply=True),\n",
    "                                 A.ISONoise(always_apply=True),], p=1),\n",
    "                        A.ImageCompression(quality_lower=50, always_apply=True),\n",
    "                        A.OneOf([A.GaussianBlur(always_apply=True),\n",
    "                                   A.RingingOvershoot(always_apply=True)], p=1),\n",
    "                        A.OneOf([A.Resize(32, 32, always_apply=True, interpolation=cv2.INTER_AREA),\n",
    "                                 A.Resize(32, 32, always_apply=True, interpolation=cv2.INTER_CUBIC),\n",
    "                                 A.Resize(32, 32, always_apply=True, interpolation=cv2.INTER_LINEAR)], p=1),\n",
    "                        A.OneOf([A.GaussNoise(always_apply=True),\n",
    "                                 A.ISONoise(always_apply=True)], p=1),\n",
    "                        A.ImageCompression(quality_lower=50, always_apply=True)\n",
    "                                 ])\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    np.random.shuffle(im_inx)\n",
    "    train_history = {'ssmi': list(), 'psnr': list(), 'loss_d': list(), 'loss_g': list(), 'l1_loss': list(), 'vgg_loss': list(), 'adv_loss': list()}\n",
    "    \n",
    "    for i in range(1, len(im_inx)+1):\n",
    "        try:\n",
    "            img = cv2.cvtColor(cv2.imread(im_inx[i-1], cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n",
    "            imgs.append(img)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if len(imgs) >= batchs or epoch == epochs:\n",
    "            imgs_tensor_hr = np.array(list(map(lambda x: transform_hr(image=x)['image'], imgs)), dtype=np.uint8)\n",
    "            imgs_tensor_lr = np.array(list(map(lambda x: transform_lr(image=x)['image'], imgs_tensor_hr)), dtype=np.uint8)\n",
    "\n",
    "            imgs_tensor_lr[imgs_tensor_lr >= 255] = 255\n",
    "            imgs_tensor_lr[imgs_tensor_lr <= 0] = 0\n",
    "            imgs_tensor_hr = imgs_tensor_hr / 127.5 -1\n",
    "            imgs_tensor_lr = imgs_tensor_lr / 255\n",
    "            imgs = []\n",
    "\n",
    "            imgs_tensor_hr = tf.cast(imgs_tensor_hr, dtype=tf.float32)\n",
    "            imgs_tensor_lr = tf.cast(imgs_tensor_lr, dtype=tf.float32)\n",
    "\n",
    "            # Upate D\n",
    "            imgs_tensor_sr = Generator(imgs_tensor_lr)\n",
    "            with tf.GradientTape() as tape:\n",
    "\n",
    "                # unfreeze bn\n",
    "                Discriminator.trainable = True\n",
    "                for layer in Discriminator.layers:\n",
    "                    if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "                        layer.trainable = True\n",
    "\n",
    "                hr_disc = Discriminator(imgs_tensor_hr)\n",
    "                sr_disc = Discriminator(imgs_tensor_sr)\n",
    "                D_RF = tf.sigmoid(hr_disc - tf.reduce_mean(sr_disc))\n",
    "                D_FR = tf.sigmoid(sr_disc - tf.reduce_mean(hr_disc))\n",
    "                loss_d = (bce(tf.ones_like(input=D_RF), D_RF) + bce(tf.zeros_like(input=D_FR), D_FR))/2\n",
    "\n",
    "            optim_d.minimize(loss_d, Discriminator.trainable_variables, tape = tape)\n",
    "\n",
    "            #Update G\n",
    "            with tf.GradientTape() as tape:\n",
    "                \n",
    "                # freeze bn\n",
    "                Discriminator.trainable = False\n",
    "                for layer in Discriminator.layers:\n",
    "                    if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "                        layer.trainable = False\n",
    "\n",
    "                imgs_tensor_sr = Generator(imgs_tensor_lr)\n",
    "                hr_disc = Discriminator(imgs_tensor_hr, training=False)\n",
    "                sr_disc = Discriminator(imgs_tensor_sr, training=False)\n",
    "                D_RF = tf.sigmoid(hr_disc - tf.reduce_mean(sr_disc))\n",
    "                D_FR = tf.sigmoid(sr_disc - tf.reduce_mean(hr_disc))\n",
    "                adv_loss = (bce(tf.zeros_like(input=D_RF), D_RF) + bce(tf.ones_like(input=D_FR), D_FR)) /2\n",
    "                \n",
    "                imgs_tensor_sr_feature_map = feature_extractor(127.5 * imgs_tensor_sr + 127.5) / 12.75\n",
    "                imgs_tensor_hr_feature_map = feature_extractor(127.5 * imgs_tensor_hr + 127.5) / 12.75\n",
    "                vgg_loss = mse(imgs_tensor_sr_feature_map, imgs_tensor_hr_feature_map)\n",
    "\n",
    "                l1_loss = mae(imgs_tensor_sr, imgs_tensor_hr)\n",
    "\n",
    "                loss_g = adv_loss * 0.005 + vgg_loss + l1_loss * 0.01\n",
    "\n",
    "            optim_g.minimize(loss_g, Generator.trainable_variables, tape=tape)\n",
    "\n",
    "            imgs_tensor_sr = imgs_tensor_sr.numpy()\n",
    "            imgs_tensor_sr = (imgs_tensor_sr + 1) / 2\n",
    "            imgs_tensor_sr[imgs_tensor_sr > 1] = 1\n",
    "            imgs_tensor_sr[imgs_tensor_sr < 0] = 0\n",
    "            imgs_tensor_hr = (imgs_tensor_hr + 1) / 2\n",
    "\n",
    "            #  학습중인 이미지 보여주기\n",
    "            sample_img = np.concatenate([cv2.resize(np.array(imgs_tensor_lr[0]), dsize=(128, 128), interpolation=cv2.INTER_CUBIC), \n",
    "                                        imgs_tensor_sr[0], \n",
    "                                        imgs_tensor_hr[0]],\n",
    "                                        axis=1)\n",
    "            sample_img = cv2.cvtColor(sample_img, cv2.COLOR_BGR2RGB)\n",
    "            sample_img = cv2.resize(sample_img, dsize=(900,300), interpolation=cv2.INTER_LINEAR)\n",
    "            cv2.imshow(winname = 'sample', mat=sample_img)\n",
    "            cv2.waitKey(1)\n",
    "\n",
    "            if iter_count % 100 == 0:\n",
    "                Generator.save('Generator.h5')\n",
    "                Discriminator.save('Discriminator.h5')\n",
    "\n",
    "            train_history['loss_d'].append(round(float(loss_d), 5))\n",
    "            train_history['adv_loss'].append(round(float(adv_loss), 5))\n",
    "            train_history['vgg_loss'].append(round(float(vgg_loss), 5))\n",
    "            train_history['l1_loss'].append(round(float(l1_loss), 5))\n",
    "            train_history['loss_g'].append(round(float(loss_g), 5))\n",
    "            train_history['ssmi'].append(round(float(np.mean(tf.image.ssim(imgs_tensor_sr, imgs_tensor_hr, max_val = 1).numpy())), 5))\n",
    "            train_history['psnr'].append(round(float(np.mean(tf.image.psnr(imgs_tensor_sr, imgs_tensor_hr, max_val = 1).numpy())), 5))\n",
    "\n",
    "            print(\"\\repoch, iteration:\", epoch, iter_count, \", step:\", i, len(im_inx), \", loss_g:\", train_history['loss_g'][-1], \",loss_d\", train_history['loss_d'][-1], \"ssim:\", train_history['ssmi'][-1], \", psnr:\", train_history['psnr'][-1], end=\"\")\n",
    "            train_history['ssmi'].append(np.mean(tf.image.ssim(imgs_tensor_sr, imgs_tensor_hr, max_val = 1).numpy()))\n",
    "            \n",
    "            # lr halved\n",
    "            if iter_count in [50000, 100000, 200000, 300000]:\n",
    "                optim_d.lr = optim_d.lr / 2\n",
    "                optim_g.lr = optim_g.lr / 2\n",
    "                print(f\"lr chaged \", optim_d.lr.numpy(), optim_g.lr.numpy())\n",
    "\n",
    "            iter_count += 1\n",
    "\n",
    "    Generator.save('Generator.h5')\n",
    "    Discriminator.save('Discriminator.h5')\n",
    "\n",
    "    print(\"\\nepochs:\", epoch, \n",
    "          'iteration', iter_count,\n",
    "          'ssmi mean:', round(np.mean(train_history['ssmi']), 5), \n",
    "          'psnr mean:', round(np.mean(train_history['psnr']), 5), \n",
    "          'loss_d', round(np.mean(train_history['loss_d']), 5), \n",
    "          'loss_g', round(np.mean(train_history['loss_g']), 5), \n",
    "          'adv_loss', round(np.mean(train_history['adv_loss']), 5), \n",
    "          'vgg_loss', round(np.mean(train_history['vgg_loss']), 5), \n",
    "          'l1_loss', round(np.mean(train_history['l1_loss']), 5))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
